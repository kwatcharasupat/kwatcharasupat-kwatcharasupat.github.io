
@misc{Watcharasupat2021ImprovingPolyphonicSound,
	title        = {
		Improving {{Polyphonic Sound Event Detection}} on {{Multichannel Recordings}}
		with the {{S{\o}rensen-Dice Coefficient Loss}} and {{Transfer Learning}}
	},
	author       = {
		Watcharasupat, Karn N. and Nguyen, Thi Ngoc Tho and Nguyen, Ngoc Khanh and
		Lee, Zhen Jian and Jones, Douglas L and Gan, Woon Seng
	},
	year         = 2021,
	bibtex_show = {true},
	dimensions = {true},
	altmetric = {true},
}


@misc{Fuentes2021SoundataSoundataV0,
    title = {Soundata/Soundata: V0.1.1},
    author = {
		Fuentes, Magdalena and Salamon, Justin and Bittner, Rachel and Roman, Iran R.
		and Plaja, Gen{\'i}s and Zinemanas, Pablo and Rubinstein, David and {Pedro}
		and Miron, Marius and Jansson, Andreas and {Thor} and Choi, Keunwoo and
		Watcharasupat, Karn and Xi, TomXi {\textbar} Qingyang and Scibor, Michael and
		{Janne} and Lee, Kyungyun and Oyama, Takehisa
	},
    year = 2021,
    month = nov,
    doi = {10.5281/ZENODO.5719430},
    bibtex_show = {true},
    dimensions = {true},
    altmetric = {true}
}


@misc{Oberman2023SoundscapeAttributesTranslation,
    title = {Soundscape Attributes Translation Project (SATP) Dataset},
    author = {
		Oberman, Tin and Mitchell, Andrew and Aletta, Francesco and Almagro Pastor,
		Jos{\'e} Antonio and Jambro{\v s}i{\'c}, Kristian and Kang, Jian
	},
    year = 2023,
    month = nov,
    abstract = {
		The data and audio included here were collected for the Soundscape Attributes
		Translation Project (SATP). First introduced in Aletta et. al. (2020), the
		SATP is an attempt to provide validated translations of soundscape attributes
		in languages other than English. The recordings were~used for headphones -
		based listening experiments. The data are provided to accompany publications
		resulting from this project and to provide a unique dataset of 1000s of
		perceptual responses to a standardised set of urban soundscape recordings.
		This dataset is the result of efforts from hundreds of researchers, students,
		assistants, PIs, and participants from institutions around the world. We have
		made an attempt to list every contributor to this Zenodo repo; if you feel
		you should be included, please get in touch. Citation: If you use the SATP
		dataset or part of it, please cite our paper describing the data collection
		and this dataset itself. Overview:~The SATP dataset consists of 27 30-sec
		binaural audio recordings made in urban public spaces in London and one 60
		sec stereo calibration signal. The recordings were made at locations as
		reported in Table 1 of the README.md (Recording locations),~at various times
		of day by an operator wearing a binaural~kit consisting of~BHS II microphones
		and a SQobold (HEAD acoustics) device. Recordings were~then exported to WAV
		via the ArtemiS SUITE software, using the original dynamic range from HDF.
		The listening experiment and the calibration procedure were intended for a
		headphone playback system (Sennheiser HD650 or similar open-back headphones
		recommended).~ The recordings were selected from an initial set of 80
		recordings through a pilot study to ensure the test set had an even coverage
		of the soundscape circumplex space. These recordings were sent to the partner
		institutions (see Table 2 of the README.md) and assessed by approximately 30
		participants in the institution's target language. The questionnaire used in
		each assessment is a translation of Method A Questionnaire, ISO 12913-2:2018.
		Each institution carried out their own lab experiment to collect data, then
		submitted their data to the team at UCL to compile into a single dataset.
		Some institutions included additional questions or translation options; the
		combined dataset (`SATP Dataset v1.x.xlsx`) includes only the base set of
		questions, the extended set of questions from each institution is included in
		the `Institution Datasets` folder. In all, SATP Dataset v1.4 contains 19,089
		samples, including 707 participants, for 27 recordings, in 18 languages with
		contributions from 29 institutions. Format:~The audio recordings are provided
		as 24 bit, 48 kHz, stereo WAV files. The combined dataset and Institutional
		datasets are provided as long tidy data tables in .xlsx files.
		Calibration:~The recommended calibration~approach was based on the
		open-circuit voltage (OCV) procedure which was considered most~accessible but
		other~calibration procedures are also possible (Lam et. al. (2022)). The
		provided calibration file is a computer generated sine wave at 1kHz,
		matching~a~sine wave recorded using the exact same setup at~SPL of 94 dB. In
		case of the calibration signal~playback level set to match~SPL of 94 dB at
		the eardrum, all the 27 samples should be reproduced at realistic
		loudness.~More details on OCV calibration procedure and other options~you can
		find in~Lam et. al. (2022) and the attached documentation. PLEASE DO NOT
		EXPOSE YOURSELF NOR THE PARTICIPANTS TO THE CALIBRATION SIGNAL SET AT THE
		REALISTIC LEVEL AS IT~CAN CAUSE HARM. License and reuse:~All SATP recordings
		are provided under the Creative Commons Attribution 4.0 International (CC BY
		4.0) License and are free to use. We encourage other researchers to replicate
		the SATP protocol and contribute new languages to the dataset. We also
		encourage the use of these recordings and the perceptual data for further
		soundscape research purposes. Please provide the proper attribution and get
		in touch with the authors if you would like to contribute a new translation
		or for any other collaborations.
	},
    publisher = {Zenodo},
    doi = {10.5281/zenodo.10159673},
    bibtex_show = {true},
    dimensions = {true},
    altmetric = {true}
}