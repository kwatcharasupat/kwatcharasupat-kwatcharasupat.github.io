<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Karn N. Watcharasupat </title> <meta name="author" content="Karn N. Watcharasupat"> <meta name="description" content="publications in reversed chronological order. generated by jekyll-scholar. the satp consortium is an international collaboration of 50+ researchers."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, music-informatics, machine-learning, signal-processing, deep-learning "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kwatcharasupat.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Karn</span> N. Watcharasupat </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications in reversed chronological order. generated by jekyll-scholar. the satp consortium is an international collaboration of 50+ researchers.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div id="Ooi2024LionCitySoundscapes" class="col-sm-10"> <div class="title"> Lion City Soundscapes: Modified Partitioning around Medoids for a Perceptually Diverse Dataset of Singaporean Soundscapes </div> <div class="author"> Kenneth Ooi, Jessie Goh, Hao-Weng Lin, Zhen-Ting Ong, Trevor Wong, <em>Karn N. Watcharasupat</em>, Bhan Lam, and Woon-Seng Gan </div> <div class="periodical"> <em>JASA Express Letters</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.21979/N9/AVHSBX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/4FAZKA82/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://researchdata.ntu.edu.sg/dataset.xhtml?persistentId=doi:10.21979/N9/AVHSBX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1121/10.0025830"></span> <span class="__dimensions_badge_embed__" data-doi="10.1121/10.0025830" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> This study presents a dataset of audio-visual soundscape recordings at 62 different locations in Singapore, initially made as full-length recordings over spans of 9–38 min. For consistency and reduction in listener fatigue in future subjective studies, one-minute excerpts were cropped from the full-length recordings. An automated method using pre-trained models for Pleasantness and Eventfulness (according to ISO 12913) in a modified partitioning around medoids algorithm was employed to generate the set of excerpts by balancing the need to encompass the perceptual space with uniformity in distribution. A validation study on the method confirmed its adherence to the intended design.. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ooi2024LionCitySoundscapes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Lion City Soundscapes: {{Modified}} Partitioning around Medoids for a
  		Perceptually Diverse Dataset of {{Singaporean}} Soundscapes
  	}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Lion City Soundscapes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Goh, Jessie and Lin, Hao-Weng and Ong, Zhen-Ting and Wong, Trevor and Watcharasupat, Karn N. and Lam, Bhan and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{JASA Express Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{047402}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1121/10.0025830}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2691-1191}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2024QuantifyingSpatialAudio" class="col-sm-10"> <div class="title">Quantifying Spatial Audio Quality Impairment</div> <div class="author"> <em>Karn N. Watcharasupat</em>, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 International Conference on Acoustics, Speech, and Signal Processing </em> , Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10447947" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://api.zotero.org/users/7862008/publications/items/UJ85MFYJ/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/karnwatcharasupat/spauq" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICASSP48485.2024.10447947"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICASSP48485.2024.10447947" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Spatial audio quality is a highly multifaceted concept, with many interactions between environmental, geometrical, anatomical, psychological, and contextual factors. Methods for characterization or evaluation of the geometrical components of spatial audio quality, however, remain scarce, despite being perhaps the least subjective aspect of spatial audio quality to quantify. By considering interchannel time and level differences relative to a reference signal, it is possible to construct a signal model to isolate some of the spatial distortion. By using a combination of least-square optimization and heuristics, we propose a signal decomposition method to isolate the spatial error, in terms of interchannel gain leakages and changes in relative delays, from a processed signal. This allows the computation of simple energy-ratio metrics, providing objective measures of spatial and non-spatial signal qualities, with minimal assumptions and no dataset dependency. Experiments demonstrate the robustness of the method against common spatial signal degradation introduced by, e.g., audio compression and music source separation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2024QuantifyingSpatialAudio</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quantifying Spatial Audio Quality Impairment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2024 {{International Conference}} on {{Acoustics}},
  		{{Speech}}, and {{Signal Processing}}
  	}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="nv">icassp</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seoul, Korea, Republic of}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{746--750}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP48485.2024.10447947}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798350344851}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Aletta2024AdvancingCrossculturalSoundscape" class="col-sm-10"> <div class="title"> Advancing Cross-Cultural Soundscape Research: Updates from the Soundscape Attributes Translation Project (SATP) </div> <div class="author"> Francesco Aletta, Tin Oberman, Andrew Mitchell, Jian Kang, and <a href="https://www.ucl.ac.uk/global/publications/2022/sep/soundscape-attributes-translation-project-satp" rel="external nofollow noopener" target="_blank"> SATP Consortium</a> </div> <div class="periodical"> <em>In Proceedings of the 53rd International Congress and Exposition on Noise Control Engineering </em> , Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://api.zotero.org/users/7862008/publications/items/9MVFMS7C/file/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> The Soundscape Attributes Translation Project (SATP) addresses linguistic barriers in soundscape research, translating ISO/TS 12913-2:2018 descriptors into various languages for global use. This paper presents recent updates from national working groups, including translations in Turkish, French, Indonesian, Albanian, Chinese, Japanese, Vietnamese, Dutch, and Spanish. A preliminary validation of all the current 18 SATP translations assesses cross-cultural robustness. SATP employs diverse methods for translation, fostering international collaboration. The preliminary validation evaluates the reliability and validity of translated descriptors across languages. Most languages maintain the quasi-circumplex structure of the original soundscape model. English, Arabic, Chinese (Mandarin), Croatian, Dutch, German, Greek, Indonesian, Italian, Spanish, Swedish, and Turkish achieve a “High” confidence level. French, Japanese, Korean, Malay, Portuguese, and Vietnamese show varying confidence levels, emphasizing the need for rigorous validation criteria. SATP advances global soundscape research, with updates from national working groups contributing to cross-cultural relevance. Preliminary validation results affirm the quasi-circumplex structure’s maintenance in most languages, emphasizing the project’s commitment to comprehensive and globally applicable soundscape research instruments. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Aletta2024AdvancingCrossculturalSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Advancing Cross-Cultural Soundscape Research: Updates from the {{Soundscape
  		Attributes Translation Project}} ({{SATP}})
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aletta, Francesco and Oberman, Tin and Mitchell, Andrew and Kang, Jian and SATP Consortium}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 53rd {{International Congress}} and {{Exposition}} on
  		{{Noise Control Engineering}}
  	}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2024ARAUSLargeScaleDataset" class="col-sm-10"> <div class="title"> ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes </div> <div class="author"> Kenneth Ooi, Zhen-Ting Ong, <em>Karn N. Watcharasupat</em>, Bhan Lam, Joo Young Hong, and Woon-Seng Gan </div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10050114" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/TAFFC.2023.3247914"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TAFFC.2023.3247914" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Choosing optimal maskers for existing soundscapes to effect a desired perceptual change via soundscape augmentation is non-trivial due to extensive varieties of maskers and a dearth of benchmark datasets with which to compare and develop soundscape augmentation models. To address this problem, we make publicly available the ARAUS (Affective Responses to Augmented Urban Soundscapes) dataset, which comprises a five-fold cross-validation set and independent test set totaling 25,440 unique subjective perceptual responses to augmented soundscapes presented as audio-visual stimuli. Each augmented soundscape is made by digitally adding “maskers” (bird, water, wind, traffic, construction, or silence) to urban soundscape recordings at fixed soundscape-to-masker ratios. Responses were then collected by asking participants to rate how pleasant, annoying, eventful, uneventful, vibrant, monotonous, chaotic, calm, and appropriate each augmented soundscape was, in accordance with ISO/TS 12913-2:2018. Participants also provided relevant demographic information and completed standard psychological questionnaires. We perform exploratory and statistical analysis of the responses obtained to verify internal consistency and agreement with known results in the literature. Finally, we demonstrate the benchmarking capability of the dataset by training and comparing four baseline models for urban soundscape pleasantness: a low-parameter regression model, a high-parameter convolutional neural network, and two attention-based networks in the literature.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ooi2024ARAUSLargeScaleDataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{ARAUS}}: {{A Large-Scale Dataset}} and {{Baseline Models}} of {{Affective
  		Responses}} to {{Augmented Urban Soundscapes}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Ong, Zhen-Ting and Watcharasupat, Karn N. and Lam, Bhan and Hong, Joo Young and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Affective Computing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105--120}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TAFFC.2023.3247914}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1949-3045}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2024ValidatingThaiTranslations" class="col-sm-10"> <div class="title"> Validating Thai Translations of Perceptual Soundscape Attributes: A Non-Procrustean Approach with a Procrustes Projection </div> <div class="author"> Karn N Watcharasupat, Kenneth Ooi, Bhan Lam, Zhen-Ting Ong, Sureenate Jaratjarungkiat, and Woon-Seng Gan </div> <div class="periodical"> <em>Applied Acoustics</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0003682X24001506" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2024.109999"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2024.109999" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Measurement of a psychological construct across populations without a common linguistic medium often necessitates the development of multiple translations of the psychometric tool across multiple languages, dialects, or other population-specific variations. In this follow-up (Stage 2) study, a listening test using a shared set of 27 stimuli from the Soundscape Attribute Translation Project (SATP) was conducted with Thai-speaking participants using the set of Thai translations of the eight perceptual affective quality (PAQ) descriptors selected in earlier (Stage 1) work through a structured evaluation questionnaire. Principal component analysis was performed on the listening test data to obtain a rank-two reduction of the responses with maximal explained variance. In order to align the principal component space to the two-dimensional circumplex space, this work presents a simple and numerically stable method based on the orthogonal Procrustes projection, to find the optimal two-dimensional orthogonal transform that aligns the first two principal components with the axes corresponding to Pleasantness and Eventfulness as defined in ISO/TS 12913-3:2019. Analysis of the listening test responses indicated good to excellent interrater reliability, reflecting the general comprehensibility of the translations to laypersons. Subsequent analyses yielded a two-dimensional projection with 94.4 % explained variance and near-perfect alignment of the composite Pleasantness and Eventfulness axes. Angular locations of the individual translated PAQs are located within 16^∘ of their theoretically ideal locations and preserve angular ordering, albeit with imperfect equiangularity. Cross-analysis against the results from Stage 1 showed that the structured evaluation may be partially useful in anticipating potential imperfections of the PAQ translations and their angular locations in Stage 2. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2024ValidatingThaiTranslations</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Validating {{Thai Translations}} of {{Perceptual Soundscape Attributes}}: A
  		{{Non-Procrustean Approach}} with a {{Procrustes Projection}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N and Ooi, Kenneth and Lam, Bhan and Ong, Zhen-Ting and Jaratjarungkiat, Sureenate and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2024.109999}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div id="Lam2023CrossingLinguisticCauseway" class="col-sm-10"> <div class="title"> Crossing the Linguistic Causeway: Ethnonational Differences on Soundscape Attributes in Bahasa Melayu </div> <div class="author"> Bhan Lam, Julia Chieng, Kenneth Ooi, Zhen Ting Ong, <em>Karn N. Watcharasupat</em>, Joo Young Hong, and Woon Seng Gan </div> <div class="periodical"> <em>Applied Acoustics</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.03647" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2023.109675"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2023.109675" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Despite being neighbouring countries and sharing the language of Bahasa Melayu (ISO 639-3: [Formula presented]), cultural and language education policy differences between Singapore and Malaysia led to differences in the translation of the “annoying” perceived affective quality (PAQ) attribute from English (ISO 639-3: [Formula presented]) to [Formula presented]. This study expands upon the translation of the PAQ attributes from [Formula presented] to [Formula presented] in Stage 1 of the Soundscapes Attributes Translation Project (SATP) initiative, and presents the findings of Stage 2 listening tests that investigated ethnonational differences in the translated [Formula presented] PAQ attributes and explored their circumplexity. A cross-cultural listening test was conducted with 100 [Formula presented] speakers from Malaysia and Singapore using the common SATP protocol. The analysis revealed that Malaysian participants from non-native ethnicities ([Formula presented]) showed PAQ perceptions more similar to Singapore ([Formula presented]) participants than native ethnic Malays ([Formula presented]) in Malaysia. Differences between Singapore and Malaysian groups were primarily observed in stimuli related to water features, reflecting cultural and geographical variations. Besides variations in water source-dominant stimuli perception, disparities between [Formula presented] and [Formula presented] could be mainly attributed to [Formula presented] scores. The findings also suggest that the adoption of region-specific translations, such as [Formula presented] in Singapore and [Formula presented] in Malaysia, adequately addressed differences in the [Formula presented] attribute, since significant differences were observed in one or fewer stimuli across ethnonational groups. The circumplexity analysis indicated that the quasi-circumplex model better fit the data compared to the assumed equal angle quasi-circumplex model in ISO/TS 12913-3, although deviations were observed possibly due to respondents’ unfamiliarity with the United Kingdom-centric context of the stimulus dataset. Furthermore, the alignment between Stage 2 listening tests and quantitative evaluation of attributes in Stage 1 revealed biases in the [Formula presented]–[Formula presented] dimension across ethnonational groups. This study provides insights into the perception of PAQ attributes in cross-cultural and cross-national contexts, facilitating the culturally appropriate adoption of translated PAQ attributes in soundscape evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lam2023CrossingLinguisticCauseway</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Crossing the Linguistic Causeway: {{Ethnonational}} Differences on Soundscape
  		Attributes in {{Bahasa Melayu}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Chieng, Julia and Ooi, Kenneth and Ong, Zhen Ting and Watcharasupat, Karn N. and Hong, Joo Young and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier Ltd}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{214}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2023.109675}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1872910X}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Oberman2023SoundscapeAttributesTranslation" class="col-sm-10"> <div class="title">Soundscape Attributes Translation Project (SATP) Dataset</div> <div class="author"> Tin Oberman, Andrew Mitchell, Francesco Aletta, José Antonio Almagro Pastor, Kristian Jambrošić, and Jian Kang </div> <div class="periodical"> Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.5281/zenodo.10159673"></span> <span class="__dimensions_badge_embed__" data-doi="10.5281/zenodo.10159673" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> The data and audio included here were collected for the Soundscape Attributes Translation Project (SATP). First introduced in Aletta et. al. (2020), the SATP is an attempt to provide validated translations of soundscape attributes in languages other than English. The recordings were used for headphones - based listening experiments. The data are provided to accompany publications resulting from this project and to provide a unique dataset of 1000s of perceptual responses to a standardised set of urban soundscape recordings. This dataset is the result of efforts from hundreds of researchers, students, assistants, PIs, and participants from institutions around the world. We have made an attempt to list every contributor to this Zenodo repo; if you feel you should be included, please get in touch. Citation: If you use the SATP dataset or part of it, please cite our paper describing the data collection and this dataset itself. Overview: The SATP dataset consists of 27 30-sec binaural audio recordings made in urban public spaces in London and one 60 sec stereo calibration signal. The recordings were made at locations as reported in Table 1 of the README.md (Recording locations), at various times of day by an operator wearing a binaural kit consisting of BHS II microphones and a SQobold (HEAD acoustics) device. Recordings were then exported to WAV via the ArtemiS SUITE software, using the original dynamic range from HDF. The listening experiment and the calibration procedure were intended for a headphone playback system (Sennheiser HD650 or similar open-back headphones recommended).  The recordings were selected from an initial set of 80 recordings through a pilot study to ensure the test set had an even coverage of the soundscape circumplex space. These recordings were sent to the partner institutions (see Table 2 of the README.md) and assessed by approximately 30 participants in the institution’s target language. The questionnaire used in each assessment is a translation of Method A Questionnaire, ISO 12913-2:2018. Each institution carried out their own lab experiment to collect data, then submitted their data to the team at UCL to compile into a single dataset. Some institutions included additional questions or translation options; the combined dataset (‘SATP Dataset v1.x.xlsx‘) includes only the base set of questions, the extended set of questions from each institution is included in the ‘Institution Datasets‘ folder. In all, SATP Dataset v1.4 contains 19,089 samples, including 707 participants, for 27 recordings, in 18 languages with contributions from 29 institutions. Format: The audio recordings are provided as 24 bit, 48 kHz, stereo WAV files. The combined dataset and Institutional datasets are provided as long tidy data tables in .xlsx files. Calibration: The recommended calibration approach was based on the open-circuit voltage (OCV) procedure which was considered most accessible but other calibration procedures are also possible (Lam et. al. (2022)). The provided calibration file is a computer generated sine wave at 1kHz, matching a sine wave recorded using the exact same setup at SPL of 94 dB. In case of the calibration signal playback level set to match SPL of 94 dB at the eardrum, all the 27 samples should be reproduced at realistic loudness. More details on OCV calibration procedure and other options you can find in Lam et. al. (2022) and the attached documentation. PLEASE DO NOT EXPOSE YOURSELF NOR THE PARTICIPANTS TO THE CALIBRATION SIGNAL SET AT THE REALISTIC LEVEL AS IT CAN CAUSE HARM. License and reuse: All SATP recordings are provided under the Creative Commons Attribution 4.0 International (CC BY 4.0) License and are free to use. We encourage other researchers to replicate the SATP protocol and contribute new languages to the dataset. We also encourage the use of these recordings and the perceptual data for further soundscape research purposes. Please provide the proper attribution and get in touch with the authors if you would like to contribute a new translation or for any other collaborations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Oberman2023SoundscapeAttributesTranslation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Soundscape {{Attributes Translation Project}} ({{SATP}}) {{Dataset}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Oberman, Tin and Mitchell, Andrew and Aletta, Francesco and Almagro Pastor, Jos{\'e} Antonio and Jambro{\v s}i{\'c}, Kristian and Kang, Jian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Zenodo}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.10159673}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Aletta2023PreliminaryResultsSoundscape" class="col-sm-10"> <div class="title"> Preliminary Results of the Soundscape Attributes Translation Project (SATP): Lessons Learned and next Steps </div> <div class="author"> F. Aletta, T. Oberman, A. Mitchell, J. Kang, and <a href="https://www.ucl.ac.uk/global/publications/2022/sep/soundscape-attributes-translation-project-satp" rel="external nofollow noopener" target="_blank"> SATP Consortium</a> </div> <div class="periodical"> <em>In Proceedings of the 10th Convention of the European Acoustics Association Forum Acusticum 2023 </em> , Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.61782/fa.2023.0095" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> The ISO/TS 12913-2:2018 document for soundscape data collection provides a questionnaire instrument for researchers and practitioners to use worldwide, but its applicability has been questioned, since it’s only available in English. To address the lack of research on translations of the soundscape descriptors proposed in Method A of the ISO technical specifications (i.e., vibrant, pleasant, calm, uneventful, monotonous, annoying, chaotic, eventful), an international collaboration, the Soundscape Attributes Translation Project (SATP), was initiated to translate the descriptors into several languages, using different methodological approaches, with the goal of validating the translations using standardized listening experiments. This paper presents the current state of advancement of the project, reporting on preliminary results from selected national working groups within the SATP network, as well as discussing the proposed analysis framework to validate the translations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Aletta2023PreliminaryResultsSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Preliminary Results of the {{Soundscape Attributes Translation Project}}
  		({{SATP}}): Lessons Learned and next Steps
  	}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{
  		Preliminary Results of the {{Soundscape Attributes Translation Project}}
  		({{SATP}})
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aletta, F. and Oberman, T. and Mitchell, A. and Kang, J. and {SATP Consortium}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 10th {{Convention}} of the {{European Acoustics
  		Association Forum Acusticum}} 2023
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Acoustics Association}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Turin, Italy}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{701--705}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.61782/fa.2023.0095}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-88-88942-67-4}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Lam2023PreliminaryInvestigationShortterm" class="col-sm-10"> <div class="title"> Preliminary Investigation of the Short-Term in Situ Performance of an Automatic Masker Selection System </div> <div class="author"> Bhan Lam, Kenneth Ooi, Zhen-Ting Ong, Trevor Wong, Woon-Seng Gan, and <em>Karn Watcharasupat</em> </div> <div class="periodical"> <em>In Proceedings of the 52nd International Congress and Exposition on Noise Control Engineering </em> , Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in_2023_0805"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in_2023_0805" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lam2023PreliminaryInvestigationShortterm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Preliminary Investigation of the Short-Term in Situ Performance of an
  		Automatic Masker Selection System
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ooi, Kenneth and Ong, Zhen-Ting and Wong, Trevor and Gan, Woon-Seng and Watcharasupat, Karn}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 52nd {{International Congress}} and {{Exposition}} on
  		{{Noise Control Engineering}}
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in_2023_0805}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ong2023EffectMaskerSelection" class="col-sm-10"> <div class="title"> Effect of Masker Selection Schemes on the Perceived Affective Quality of Soundscapes: A Pilot Study </div> <div class="author"> Zhen-Ting Ong, Kenneth Ooi, Trevor Wong, Bhan Lam, Woon-Seng Gan, and <em>Karn N. Watcharasupat</em> </div> <div class="periodical"> <em>In Proceedings of the 52nd International Congress and Exposition on Noise Control Engineering </em> , Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in_2023_0791"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in_2023_0791" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ong2023EffectMaskerSelection</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Effect of Masker Selection Schemes on the Perceived Affective Quality of
  		Soundscapes: {{A}} Pilot Study
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ong, Zhen-Ting and Ooi, Kenneth and Wong, Trevor and Lam, Bhan and Gan, Woon-Seng and Watcharasupat, Karn N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 52nd {{International Congress}} and {{Exposition}} on
  		{{Noise Control Engineering}}
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in_2023_0791}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2023ARAUSv2ExpandedDataset" class="col-sm-10"> <div class="title"> ARAUSv2: An Expanded Dataset and Multimodal Models of Affective Responses to Augmented Urban Soundscapes </div> <div class="author"> Kenneth Ooi, Zhen-Ting Ong, Bhan Lam, Trevor Wong, Woon-Seng Gan, and <em>Karn Watcharasupat</em> </div> <div class="periodical"> <em>In Proceedings of the 52nd International Congress and Exposition on Noise Control Engineering </em> , Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in_2023_0459"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in_2023_0459" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2023ARAUSv2ExpandedDataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{ARAUSv2}}: {{An Expanded Dataset}} and {{Multimodal Models}} of {{Affective
  		Responses}} to {{Augmented Urban Soundscapes}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Ong, Zhen-Ting and Lam, Bhan and Wong, Trevor and Gan, Woon-Seng and Watcharasupat, Karn}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 52nd {{International Congress}} and {{Exposition}} on
  		{{Noise Control Engineering}}
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in_2023_0459}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2023AutonomousSoundscapeAugmentation" class="col-sm-10"> <div class="title"> Autonomous Soundscape Augmentation with Multimodal Fusion of Visual and Participant-linked Inputs </div> <div class="author"> Kenneth Ooi, Karn N Watcharasupat, Bhan Lam, Zhen-Ting Ong, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 2023 International Conference on Acoustics, Speech, and Signal Processing </em> , Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICASSP49357.2023.10094866"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICASSP49357.2023.10094866" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Autonomous soundscape augmentation systems typically use trained models to pick optimal maskers to effect a desired perceptual change. While acoustic information is paramount to such systems, contextual information, including participant demographics and the visual environment, also influences acoustic perception. Hence, we propose modular modifications to an existing attention-based deep neural network, to allow early, mid-level, and late feature fusion of participant-linked, visual, and acoustic features. Ablation studies on module configurations and corresponding fusion methods using the ARAUS dataset show that contextual features improve the model performance in a statistically significant manner on the normalized ISO Pleasantness, to a mean squared error of 0.1194\textpm0.0012 for the best-performing all-modality model, against 0.1217 \textpm 0.0009 for the audio-only model. Soundscape augmentation systems can thereby leverage multimodal inputs for improved performance. We also investigate the impact of individual participant-linked factors using trained models to illustrate improvements in model explainability. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2023AutonomousSoundscapeAugmentation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Autonomous {{Soundscape Augmentation}} with {{Multimodal Fusion}} of
  		{{Visual}} and {{Participant-linked Inputs}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N and Lam, Bhan and Ong, Zhen-Ting and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2023 {{International Conference}} on {{Acoustics}},
  		{{Speech}}, and {{Signal Processing}}
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP49357.2023.10094866}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2023GeneralizedBandsplitNeural" class="col-sm-10"> <div class="title"> A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Chih-Wei Wu, <a href="https://suncerock.github.io/" rel="external nofollow noopener" target="_blank">Yiwei Ding</a>, Iroro Orife, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a>, and William Wolcott </div> <div class="periodical"> <em>IEEE Open Journal of Signal Processing</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/OJSP.2023.3339428"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/OJSP.2023.3339428" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue, music, and effects stems from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psychoacoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-tonoise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2023GeneralizedBandsplitNeural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		A {{Generalized Bandsplit Neural Network}} for {{Cinematic Audio Source}}
  		Separation
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Wu, Chih-Wei and Ding, Yiwei and Orife, Iroro and Hipple, Aaron J. and Williams, Phillip A. and Kramer, Scott and Lerch, Alexander and Wolcott, William}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Open Journal of Signal Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{73--81}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/OJSP.2023.3339428}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2644-1322}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div id="Watcharasupat2022QuantitativeEvaluationApproach" class="col-sm-10"> <div class="title"> Quantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Sureenate Jaratjarungkiat, Bhan Lam, Sujinat Jitwiriyanont, Kenneth Ooi, Zhen Ting Ong, Nitipong Pichetpan, Kanyanut Akaratham, Titima Suthiwan, Monthita Rojtinnakorn, and Woon Seng Gan </div> <div class="periodical"> <em>Applied Acoustics</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.12245" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2022.108962"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2022.108962" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Translation of perceptual soundscape attributes from one language to another remains a challenging task that requires a high degree of fidelity in both psychoacoustic and psycholinguistic senses across the target population. Due to the inherently subjective nature of human perception, translating soundscape attributes using only small focus group discussions or expert panels could lead to translations with psycholinguistic meanings that, in a non-expert setting, deviate or distort from that of the source language. In this work, we present a quantitative evaluation method based on the circumplex model of soundscape perception to assess the overall translation quality. By establishing a set of criteria for evaluating the linguistic and psychometric properties of the translation candidates, statistical analyses can be performed to objectively assess specific strengths and weaknesses of the translation candidates before committing to listening tests or more involved validation experiments. As an initial application domain, we demonstrated the use of the quantitative evaluation framework in the context of an English-to-Thai translation of soundscape attributes. A total of 31 participants who are bilingual in English and Thai were recruited to assess the translation candidates. Subsequent statistical analysis of the evaluation scores revealed acoustico-psycholinguistic properties of the translation candidates which were not previously identified by the expert panel and facilitated a more objective selection of the final translations for subsequent usage. Additionally, with specific biases of the final translations determined numerically, mathematical and statistical techniques for corrections of the survey data may be employed in the future to improve cross-lingual compatibility in soundscape evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022QuantitativeEvaluationApproach</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Quantitative Evaluation Approach for Translation of Perceptual Soundscape
  		Attributes: {{Initial}} Application to the {{Thai Language}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Jaratjarungkiat, Sureenate and Lam, Bhan and Jitwiriyanont, Sujinat and Ooi, Kenneth and Ong, Zhen Ting and Pichetpan, Nitipong and Akaratham, Kanyanut and Suthiwan, Titima and Rojtinnakorn, Monthita and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier Ltd}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{200}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2022.108962}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1872910X}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Lam2022AssessmentCosteffectiveHeadphone" class="col-sm-10"> <div class="title"> Assessment of a Cost-Effective Headphone Calibration Procedure for Soundscape Evaluations </div> <div class="author"> Bhan Lam, Kenneth Ooi, Zhen-Ting Ong, <em>Karn N. Watcharasupat</em>, Trevor Wong, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 24th International Congress on Acoustics</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.12899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> To increase the availability and adoption of the soundscape standard, a low-cost calibration procedure for reproduction of audio stimuli over headphones was proposed as part of the global “Soundscape Attributes Translation Project” (SATP) for validating ISO/TS~12913-2:2018 perceived affective quality (PAQ) attribute translations. A previous preliminary study revealed significant deviations from the intended equivalent continuous A-weighted sound pressure levels (Ł_{}text{A,eq}}\) using the open-circuit voltage (OCV) calibration procedure. For a more holistic human-centric perspective, the OCV method is further investigated here in terms of psychoacoustic parameters, including relevant exceedance levels to account for temporal effects on the same 27 stimuli from the SATP. Moreover, a within-subjects experiment with 36 participants was conducted to examine the effects of OCV calibration on the PAQ attributes in ISO/TS~12913-2:2018. Bland-Altman analysis of the objective indicators revealed large biases in the OCV method across all weighted sound level and loudness indicators; and roughness indicators at }SI{5}{}%} and }SI{10}{}%} exceedance levels. Significant perceptual differences due to the OCV method were observed in about }SI{20}{}%} of the stimuli, which did not correspond clearly with the biased acoustic indicators. A cautioned interpretation of the objective and perceptual differences due to small and unpaired samples nevertheless provide grounds for further investigation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lam2022AssessmentCosteffectiveHeadphone</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Assessment of a Cost-Effective Headphone Calibration Procedure for Soundscape
  		Evaluations
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ooi, Kenneth and Ong, Zhen-Ting and Watcharasupat, Karn N. and Wong, Trevor and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th {{International Congress}} on {{Acoustics}}}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2022SingaporeSoundscapeSite" class="col-sm-10"> <div class="title"> Singapore Soundscape Site Selection Survey (S5): Identification of Characteristic Soundscapes of Singapore via Weighted k-Means Clustering </div> <div class="author"> Kenneth Ooi, Bhan Lam, Joo Young Hong, <em>Karn N. Watcharasupat</em>, Zhen Ting Ong, and Woon Seng Gan </div> <div class="periodical"> <em>Sustainability</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3390/su14127485"></span> <span class="__dimensions_badge_embed__" data-doi="10.3390/su14127485" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> The ecological validity of soundscape studies usually rests on the choice of soundscapes that are representative of the perceptual space under investigation. For example, a soundscape pleasantness study might investigate locations with soundscapes ranging from “pleasant” to “annoying”. The choice of soundscapes is typically researcher led, but a participant-led process can reduce selection bias and improve result reliability. Hence, we propose a robust participant-led method to pinpoint characteristic soundscapes possessing arbitrary perceptual attributes. We validate our method by identifying Singaporean soundscapes spanning the perceptual quadrants generated from the “Pleasantness” and “Eventfulness” axes of the ISO 12913-2 circumplex model of soundscape perception, as perceived by local experts. From memory and experience, 67 participants first selected locations corresponding to each perceptual quadrant in each major planning region of Singapore. We then performed weighted k-means clustering on the selected locations, with weights for each location derived from previous frequencies and durations spent in each location by each participant. Weights hence acted as proxies for participant confidence. In total, 62 locations were thereby identified as suitable locations with characteristic soundscapes for further research utilizing the ISO 12913-2 perceptual quadrants. Audio–visual recordings and acoustic characterization of the soundscapes will be made in a future study. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ooi2022SingaporeSoundscapeSite</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Singapore {{Soundscape Site Selection Survey}} ({{S5}}): {{Identification}}
  		of {{Characteristic Soundscapes}} of {{Singapore}} via {{Weighted}} k-{{Means
  		Clustering}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Lam, Bhan and Hong, Joo Young and Watcharasupat, Karn N. and Ong, Zhen Ting and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sustainability}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/su14127485}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{20711050}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Lam2022PreliminaryAssessmentCosteffective" class="col-sm-10"> <div class="title"> Preliminary Assessment of a Cost-Effective Headphone Calibration Procedure for Soundscape Evaluations </div> <div class="author"> Bhan Lam, Kenneth Ooi, <em>Karn N. Watcharasupat</em>, Zhen-Ting Ong, Yun-Ting Lau, Trevor Wong, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 28th International Congress on Sound and Vibration </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.04728" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> The introduction of ISO 12913-2:2018 has provided a framework for standardized data collection and reporting procedures for soundscape practitioners. A strong emphasis was placed on the use of calibrated head and torso simulators (HATS) for binaural audio capture to obtain an accurate subjective impression and acoustic measure of the soundscape under evaluation. To auralise the binaural recordings as recorded or at set levels, the audio stimuli and the headphone setup are usually calibrated with a HATS. However, calibrated HATS are too financially prohibitive for most research teams, inevitably diminishing the availability of the soundscape standard. With the increasing availability of soundscape binaural recording datasets, and the importance of cross-cultural validation of the soundscape ISO standards, e.g.} via the Soundscape Attributes Translation Project (SATP), it is imperative to assess the suitability of cost-effective headphone calibration methods to maximise availability without severely compromising on accuracy. Hence, this study objectively examines an open-circuit voltage (OCV) calibration method in comparison to a calibrated HATS on various soundcard and headphone combinations. Preliminary experiments found that calibration with the OCV method differed significantly from the reference binaural recordings in sound pressure levels, whereas negligible differences in levels were observed with the HATS calibration. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lam2022PreliminaryAssessmentCosteffective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Preliminary Assessment of a Cost-Effective Headphone Calibration Procedure
  		for Soundscape Evaluations
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Ooi, Kenneth and Watcharasupat, Karn N. and Ong, Zhen-Ting and Lau, Yun-Ting and Wong, Trevor and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 28th {{International Congress}} on {{Sound}} and
  		{{Vibration}}
  	}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Lam2022CrossingLinguisticCauseway" class="col-sm-10"> <div class="title"> Crossing the Linguistic Causeway: A Binational Approach for Translating Soundscape Attributes to Bahasa Melayu </div> <div class="author"> Bhan Lam, Julia Chieng, Karn N Watcharasupat, Kenneth Ooi, Zhen-Ting Ong, Joo Young Hong, and Woon-Seng Gan </div> <div class="periodical"> <em>Applied Acoustics</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.apacoust.2022.108976"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.apacoust.2022.108976" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Translation of perceptual descriptors such as the perceived affective quality attributes in the soundscape standard (ISO/TS 12913–2:2018) is an inherently intricate task, especially if the target language is used in multiple countries. Despite geographical proximity and a shared language of Bahasa Melayu (Standard Malay), differences in culture and language education policies between Singapore and Malaysia could invoke peculiarities in the affective appraisal of sounds. To generate provisional translations of the eight perceived affective attributes — eventful, vibrant, pleasant, calm, uneventful, monotonous, annoying, and chaotic — into Bahasa Melayu that is applicable in both Singapore and Malaysia, a binational expert-led approach supplemented by a quantitative evaluation framework was adopted. A set of preliminary translation candidates were developed via a four-stage process, firstly by a qualified translator, which was then vetted by linguistics experts, followed by examination via an experiential evaluation, and finally reviewed by the core research team. A total of 66 participants were then recruited cross-nationally to quantitatively evaluate the preliminary translation candidates. Of the eight attributes, cross-national differences were observed only in the translation of annoying. For instance, menjengkelkan was found to be significantly less understood in Singapore than in Malaysia, as well as less understandable than membingitkan within Singapore. Results of the quantitative evaluation also revealed the imperfect nature of foreign language translations for perceptual descriptors, which suggests a possibility for exploring corrective measures. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lam2022CrossingLinguisticCauseway</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Crossing the Linguistic Causeway: {{A}} Binational Approach for Translating
  		Soundscape Attributes to {{Bahasa Melayu}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lam, Bhan and Chieng, Julia and Watcharasupat, Karn N and Ooi, Kenneth and Ong, Zhen-Ting and Hong, Joo Young and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Acoustics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{199}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{108976}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.apacoust.2022.108976}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0003-682X}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Nguyen2022SALSALiteFastEffective" class="col-sm-10"> <div class="title"> SALSA-Lite: A Fast and Effective Feature for Polyphonic Sound Event Localization and Detection with Microphone Arrays </div> <div class="author"> T N Tho Nguyen, D L Jones, <em>Karn N. Watcharasupat</em>, H Phan, and W -S. Gan </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9746132"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9746132" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Nguyen2022SALSALiteFastEffective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{SALSA-Lite}}: {{A Fast}} and {{Effective Feature}} for {{Polyphonic Sound
  		Event Localization}} and {{Detection}} with {{Microphone Arrays}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, T N Tho and Jones, D L and Watcharasupat, Karn N. and Phan, H and Gan, W -S.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 {{IEEE International Conference}} on {{Acoustics}},
  		{{Speech}} and {{Signal Processing}} ({{ICASSP}})
  	}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{716--720}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9746132}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{2379-190X}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Nguyen2022SALSASpatialCueAugmented" class="col-sm-10"> <div class="title"> SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection </div> <div class="author"> T N T Nguyen, <em>Karn N. Watcharasupat</em>, N K Nguyen, D L Jones, and W -S. Gan </div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/taslp.2022.3173054"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/taslp.2022.3173054" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Nguyen2022SALSASpatialCueAugmented</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{SALSA}}: {{Spatial Cue-Augmented Log-Spectrogram Features}} for
  		{{Polyphonic Sound Event Localization}} and {{Detection}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, T N T and Watcharasupat, Karn N. and Nguyen, N K and Jones, D L and Gan, W -S.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1749--1762}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/taslp.2022.3173054}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2329-9304}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ong2022UHearValidationUHear" class="col-sm-10"> <div class="title"> Do uHear? Validation of uHear App for Preliminary Screening of Hearing Ability in Soundscape Studies </div> <div class="author"> Zhen-Ting Ong, Bhan Lam, Kenneth Ooi, Karn N Watcharasupat, Trevor Wong, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 24th International Congress on Acoustics</em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.21979/n9/jqdi6f"></span> <span class="__dimensions_badge_embed__" data-doi="10.21979/n9/jqdi6f" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Studies involving soundscape perception often exclude participants with hearing loss to prevent impaired perception from affecting experimental results. Participants are typically screened with pure tone audiometry, the "gold standard" for identifying and quantifying hearing loss at specific frequencies, and excluded if a study-dependent threshold is not met. However, procuring professional audiometric equipment for soundscape studies may be cost-ineffective, and manually performing audiometric tests is labour-intensive. Moreover, testing requirements for soundscape studies may not require sensitivities and specificities as high as that in a medical diagnosis setting. Hence, in this study, we investigate the effectiveness of the uHear app, an iOS application, as an affordable and automatic alternative to a conventional audiometer in screening participants for hearing loss for the purpose of soundscape studies or listening tests in general. Based on audiometric comparisons with the audiometer of 163 participants, the uHear app was found to have high precision (98.04 %) when using the World Health Organization (WHO) grading scheme for assessing normal hearing. Precision is further improved (98.69 %) when all frequencies assessed with the uHear app is considered in the grading, which lends further support to this cost-effective, automated alternative to screen for normal hearing. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ong2022UHearValidationUHear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Do {{uHear}}? {{Validation}} of {{uHear App}} for {{Preliminary Screening}}
  		of {{Hearing Ability}} in {{Soundscape Studies}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ong, Zhen-Ting and Lam, Bhan and Ooi, Kenneth and Watcharasupat, Karn N and Wong, Trevor and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th {{International Congress}} on {{Acoustics}}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21979/n9/jqdi6f}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2022BenchmarkComparisonPerceptual" class="col-sm-10"> <div class="title"> A Benchmark Comparison of Perceptual Models for Soundscapes on a Large-Scale Augmented Soundscape Dataset </div> <div class="author"> Kenneth Ooi, <em>Karn N. Watcharasupat</em>, Bhan Lam, Zhen-Ting Ong, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 24th International Congress on Acoustics</em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.21979/n9/9otevx"></span> <span class="__dimensions_badge_embed__" data-doi="10.21979/n9/9otevx" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2022BenchmarkComparisonPerceptual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		A Benchmark Comparison of Perceptual Models for Soundscapes on a Large-Scale
  		Augmented Soundscape Dataset
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Lam, Bhan and Ong, Zhen-Ting and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th {{International Congress}} on {{Acoustics}}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21979/n9/9otevx}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2022ProbablyPleasantNeuralProbabilistic" class="col-sm-10"> <div class="title"> Probably Pleasant? A Neural-Probabilistic Approach to Automatic Masker Selection for Urban Soundscape Augmentation </div> <div class="author"> Kenneth Ooi, <em>Karn N. Watcharasupat</em>, Bhan Lam, Zhen-Ting Ong, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9746897"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9746897" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2022ProbablyPleasantNeuralProbabilistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Probably {{Pleasant}}? {{A Neural-Probabilistic Approach}} to {{Automatic
  		Masker Selection}} for {{Urban Soundscape Augmentation}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Lam, Bhan and Ong, Zhen-Ting and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 {{IEEE International Conference}} on {{Acoustics}},
  		{{Speech}} and {{Signal Processing}}
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9746897}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2022AutonomousInSituSoundscape" class="col-sm-10"> <div class="title"> Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Kenneth Ooi, Bhan Lam, Trevor Wong, Zhen Ting Ong, and Woon Seng Gan </div> <div class="periodical"> <em>IEEE Signal Processing Letters</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.13883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/lsp.2022.3194419"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/lsp.2022.3194419" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> The selection of maskers and playback gain levels in an in-situ soundscape augmentation system is crucial to its effectiveness in improving the overall acoustic comfort of a given environment. Traditionally, the selection of appropriate maskers and gain levels has been informed by expert opinion, which may not be representative of the target population, or by listening tests, which can be time- and labor-intensive. Furthermore, the resulting static choices of masker and gain are often inflexible to dynamic real-world soundscapes. In this work, we utilized a deep learning model to perform joint selection of the optimal masker and its gain level for a given soundscape. The proposed model was designed with highly modular building blocks, allowing for an optimized inference process that can quickly search through a large number of masker-gain combinations. In addition, we introduced the use of feature-domain soundscape augmentation conditioned on the digital gain level, eliminating the computationally expensive waveform-domain mixing process during inference, as well as the tedious gain adjustment process required for new maskers. The proposed system was evaluated on a large-scale dataset of subjective responses to augmented soundscapes with 442 participants, with the best model achieving a mean squared error of <inline-formula><tex-math notation="LaTeX">}{0.122}}mathbf {}pm }{0.005}{</tex-math></inline-formula> on pleasantness score, validating the ability of the model to predict combined effect of the masker and its gain level on the perceptual pleasantness level. The proposed system thus allows in-situ or mixed-reality soundscape augmentation to be performed autonomously with near real-time latency while continuously accounting for changes in acoustic environments. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022AutonomousInSituSoundscape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Autonomous {{In-Situ Soundscape Augmentation}} via {{Joint Selection}} of
  		{{Masker}} and {{Gain}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Ooi, Kenneth and Lam, Bhan and Wong, Trevor and Ong, Zhen Ting and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1749--1753}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/lsp.2022.3194419}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{15582361}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2022EndtoEndComplexValuedMultidilated" class="col-sm-10"> <div class="title"> End-to-End Complex-Valued Multidilated Convolutional Neural Network for Joint Acoustic Echo Cancellation and Noise Suppression </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Thi Ngoc Tho Nguyen, Woon-Seng Gan, Shengkui Zhao, and Bin Ma </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.00745" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9747034"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9747034" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Echo and noise suppression is an integral part of a full-duplex communication system. Many recent acoustic echo cancellation (AEC) systems rely on a separate adaptive filtering module for linear echo suppression and a neural module for residual echo suppression. However, not only do adaptive filtering modules require convergence and remain susceptible to changes in acoustic environments, but this two-stage framework also often introduces unnecessary delays to the AEC system when neural modules are already capable of both linear and nonlinear echo suppression. In this paper, we exploit the offset-compensating ability of complex time-frequency masks and propose an end-to-end complex-valued neural network architecture. The building block of the proposed model is a pseudocomplex extension based on the densely-connected multidilated DenseNet (D3Net) building block, resulting in a very small network of only 354K parameters. The architecture utilized the multi-resolution nature of the D3Net building blocks to eliminate the need for pooling, allowing the network to extract features using large receptive fields without any loss of output resolution. We also propose a dual-mask technique for joint echo and noise suppression with simultaneous speech enhancement. Evaluation on both synthetic and real test sets demonstrated promising results across multiple energy-based metrics and perceptual proxies. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2022EndtoEndComplexValuedMultidilated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		End-to-{{End Complex-Valued Multidilated Convolutional Neural Network}} for
  		{{Joint Acoustic Echo Cancellation}} and {{Noise Suppression}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Zhao, Shengkui and Ma, Bin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 {{IEEE International Conference}} on {{Acoustics}},
  		{{Speech}} and {{Signal Processing}}
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{656--660}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9747034}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-66540-540-9}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2022LatteCrossframeworkPython" class="col-sm-10"> <div class="title"> Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models </div> <div class="author"> Karn N Watcharasupat, Junyoung Lee, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>Software Impacts</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.simpa.2022.100222"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.simpa.2022.100222" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Latte (for LATent Tensor Evaluation) is a Python library for evaluation of latent-based generative models in the fields of disentanglement learning and controllable generation. Latte is compatible with both PyTorch and TensorFlow/Keras, and provides both functional and modular APIs that can be easily extended to support other deep learning frameworks. Using NumPy-based and framework-agnostic implementation, Latte ensures reproducible, consistent, and deterministic metric calculations regardless of the deep learning framework of choice. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watcharasupat2022LatteCrossframeworkPython</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Latte: {{Cross-framework Python}} Package for Evaluation of Latent-Based
  		Generative Models
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N and Lee, Junyoung and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Software Impacts}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{100222}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.simpa.2022.100222}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2665-9638}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Wong2022DeploymentIoTSystem" class="col-sm-10"> <div class="title"> Deployment of an IoT System for Adaptive In-Situ Soundscape Augmentation </div> <div class="author"> Trevor Wong, <em>Karn N. Watcharasupat</em>, Bhan Lam, Kenneth Ooi, Zhen-Ting Ong, Furi Andi Karnapi, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 51st International Congress and Expo on Noise Control Engineering </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.13890" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/IN_2022_0290"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/IN_2022_0290" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Soundscape augmentation is an emerging approach for noise mitigation by introducing additional sounds known as "maskers" to increase acoustic comfort. Traditionally, the choice of maskers is often predicated on expert guidance or post-hoc analysis which can be time-consuming and sometimes arbitrary. Moreover, this often results in a static set of maskers that are inflexible to the dynamic nature of real-world acoustic environments. Overcoming the inflexibility of traditional soundscape augmentation is twofold. First, given a snapshot of a soundscape, the system must be able to select an optimal masker without human supervision. Second, the system must also be able to react to changes in the acoustic environment with near real-time latency. In this work, we harness the combined prowess of cloud computing and the Internet of Things (IoT) to allow in-situ listening and playback using microcontrollers while delegating computationally expensive inference tasks to the cloud. In particular, a serverless cloud architecture was used for inference, ensuring near real-time latency and scalability without the need to provision computing resources. A working prototype of the system is currently being deployed in a public area experiencing high traffic noise, as well as undergoing public evaluation for future improvements. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wong2022DeploymentIoTSystem</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Deployment of an {{IoT System}} for {{Adaptive In-Situ Soundscape
  		Augmentation}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wong, Trevor and Watcharasupat, Karn N. and Lam, Bhan and Ooi, Kenneth and Ong, Zhen-Ting and Karnapi, Furi Andi and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 51st {{International Congress}} and {{Expo}} on {{Noise
  		Control Engineering}}
  	}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/IN_2022_0290}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Zhao2022FRCRNBoostingFeature" class="col-sm-10"> <div class="title"> FRCRN: Boosting Feature Representation Using Frequency Recurrence for Monaural Speech Enhancement </div> <div class="author"> Shengkui Zhao, Bin Ma, <em>Karn N. Watcharasupat</em>, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp43922.2022.9747578"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp43922.2022.9747578" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Convolutional recurrent networks (CRN) integrating a convolutional encoder-decoder (CED) structure and a recurrent structure have achieved promising performance for monaural speech enhancement. However, feature representation across frequency context is highly constrained due to limited receptive fields in the convolutions of CED. In this paper, we propose a convolutional recurrent encoder-decoder (CRED) structure to boost feature representation along the frequency axis. The CRED applies frequency recurrence on 3D convolutional feature maps along the frequency axis following each convolution, therefore, it is capable of catching long-range frequency correlations and enhancing feature representations of speech inputs. The proposed frequency recurrence is realized efficiently using a feedforward sequential memory network (FSMN). Besides the CRED, we insert two stacked FSMN layers between the encoder and the decoder to model further temporal dynamics. We name the proposed framework as Frequency Recurrent CRN (FRCRN). We design FRCRN to predict complex Ideal Ratio Mask (cIRM) in complex-valued domain and optimize FRCRN using both time-frequency-domain and time-domain losses. Our proposed approach achieved state-of-the-art performance on wideband benchmark datasets and achieved 2nd place for the real-time fullband track in terms of Mean Opinion Score (MOS) and Word Accuracy (WAcc) in the ICASSP 2022 Deep Noise Suppression (DNS) challenge. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zhao2022FRCRNBoostingFeature</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{FRCRN}}: {{Boosting Feature Representation Using Frequency Recurrence}} for
  		{{Monaural Speech Enhancement}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Shengkui and Ma, Bin and Watcharasupat, Karn N. and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2022 {{IEEE International Conference}} on {{Acoustics}},
  		{{Speech}} and {{Signal Processing}}
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9281--9285}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9747578}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-66540-540-9}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div id="Fuentes2021SoundataSoundataV0" class="col-sm-10"> <div class="title">Soundata/Soundata: V0.1.1</div> <div class="author"> Magdalena Fuentes, Justin Salamon, Rachel Bittner, Iran R. Roman, Genís Plaja, Pablo Zinemanas, David Rubinstein,  Pedro, Marius Miron, Andreas Jansson,  Thor, Keunwoo Choi, <em>Karn Watcharasupat</em>, TomXi \textbar Qingyang Xi, Michael Scibor,  Janne, Kyungyun Lee, and Takehisa Oyama </div> <div class="periodical"> Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.5281/ZENODO.5719430"></span> <span class="__dimensions_badge_embed__" data-doi="10.5281/ZENODO.5719430" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Fuentes2021SoundataSoundataV0</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Soundata/Soundata: V0.1.1}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fuentes, Magdalena and Salamon, Justin and Bittner, Rachel and Roman, Iran R. and Plaja, Gen{\'i}s and Zinemanas, Pablo and Rubinstein, David and {Pedro} and Miron, Marius and Jansson, Andreas and {Thor} and Choi, Keunwoo and Watcharasupat, Karn and Xi, TomXi {\textbar} Qingyang and Scibor, Michael and {Janne} and Lee, Kyungyun and Oyama, Takehisa}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/ZENODO.5719430}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Hung2021AVASpeechSMADStronglyLabelled" class="col-sm-10"> <div class="title"> AVASpeech-SMAD: A Strongly Labelled Speech and Music Activity Detection Dataset with Label Co-Occurrence </div> <div class="author"> Yun-Ning Hung, <em>Karn N. Watcharasupat</em>, Chih-Wei Wu, Iroro Orife, Kelian Li, Pavan Seshadri, and Junyoung Lee </div> <div class="periodical"> <em>In Extended Abstracts of the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference </em> , Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1510.08484" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Hung2021AVASpeechSMADStronglyLabelled</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{AVASpeech-SMAD}}: {{A Strongly Labelled Speech}} and {{Music Activity
  		Detection Dataset}} with {{Label Co-Occurrence}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hung, Yun-Ning and Watcharasupat, Karn N. and Wu, Chih-Wei and Orife, Iroro and Li, Kelian and Seshadri, Pavan and Lee, Junyoung}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Extended {{Abstracts}} of the {{Late-Breaking Demo Session}} of the 22nd
  		{{International Society}} for {{Music Information Retrieval Conference}}
  	}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2021DirectionalSparseFiltering" class="col-sm-10"> <div class="title"> Directional Sparse Filtering Using Weighted Lehmer Mean for Blind Separation of Unbalanced Speech Mixtures </div> <div class="author"> <em>Karn Watcharasupat</em>, Anh H. T. Nguyen, Ching-Hui Ooi, and Andy W. H. Khong </div> <div class="periodical"> <em>In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.00196" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/icassp39728.2021.9414336"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/icassp39728.2021.9414336" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> In blind source separation of speech signals, the inherent imbalance in the source spectrum poses a challenge for methods that rely on single-source dominance for the estimation of the mixing matrix. We propose an algorithm based on the directional sparse filtering (DSF) framework that utilizes the Lehmer mean with learnable weights to adaptively account for source imbalance. Performance evaluation in multiple real acoustic environments show improvements in source separation compared to the baseline methods. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2021DirectionalSparseFiltering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Directional {{Sparse Filtering Using Weighted Lehmer Mean}} for {{Blind
  		Separation}} of {{Unbalanced Speech Mixtures}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn and Nguyen, Anh H. T. and Ooi, Ching-Hui and Khong, Andy W. H.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 2021 {{IEEE International Conference}} on {{Acoustics}},
  		{{Speech}} and {{Signal Processing}}
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4485--4489}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp39728.2021.9414336}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-72817-605-5}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{23318422}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Karnapi2021DevelopmentFeedbackInterface" class="col-sm-10"> <div class="title">Development of a Feedback Interface for In-Situ Soundscape Evaluation</div> <div class="author"> Furi Andi Karnapi, Bhan Lam, Kenneth Ooi, Yun-Ting Lau, <em>Karn Watcharasupat</em>, Trevor Wong, Woon-Seng Gan, Jooyoung Hong, Samuel Yeong, and Irene Lee </div> <div class="periodical"> <em>In Proceedings of the 50th International Congress and Expo on Noise Control Engineering </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in-2021-2084"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in-2021-2084" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Karnapi2021DevelopmentFeedbackInterface</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Development of a Feedback Interface for In-Situ Soundscape Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karnapi, Furi Andi and Lam, Bhan and Ooi, Kenneth and Lau, Yun-Ting and Watcharasupat, Karn and Wong, Trevor and Gan, Woon-Seng and Hong, Jooyoung and Yeong, Samuel and Lee, Irene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 50th {{International Congress}} and {{Expo}} on {{Noise
  		Control Engineering}}
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{I-INCE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Washington, D.C., USA}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in-2021-2084}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Nguyen2021DCASE2021Task" class="col-sm-10"> <div class="title"> DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection </div> <div class="author"> Thi Ngoc Tho Nguyen, <em>Karn Watcharasupat</em>, Ngoc Khanh Nguyen, Douglas L Jones, and Woon Seng Gan </div> <div class="periodical"> Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Nguyen2021DCASE2021Task</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		{{DCASE}} 2021 {{Task}} 3: {{Spectrotemporally-aligned Features}} for
  		{{Polyphonic Sound Event Localization}} and {{Detection}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Thi Ngoc Tho and Watcharasupat, Karn and Nguyen, Ngoc Khanh and Jones, Douglas L and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{
  		IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and
  		Events
  	}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Nguyen2021WhatMakesSound" class="col-sm-10"> <div class="title"> What Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis </div> <div class="author"> Thi Ngoc Tho Nguyen, <em>Karn N. Watcharasupat</em>, Zhen Jian Lee, Ngoc Khanh Nguyen, Douglas L Jones, and Woon Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 6th Workshop on Detection and Classification of Acoustic Scenes and Events </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Nguyen2021WhatMakesSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		What {{Makes Sound Event Localization}} and {{Detection Difficult}}?
  		{{Insights}} from {{Error Analysis}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Thi Ngoc Tho and Watcharasupat, Karn N. and Lee, Zhen Jian and Nguyen, Ngoc Khanh and Jones, Douglas L and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 6th {{Workshop}} on {{Detection}} and {{Classification}}
  		of {{Acoustic Scenes}} and {{Events}}
  	}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{November}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Ooi2021StronglyLabelledPolyphonicDataset" class="col-sm-10"> <div class="title"> A Strongly-Labelled Polyphonic Dataset of Urban Sounds with Spatiotemporal Context </div> <div class="author"> Kenneth Ooi, <em>Karn N. Watcharasupat</em>, Santi Peksi, Furi Andi Karnapi, Zhen-Ting Ong, Danny Chua, Hui-Wen Leow, Li-Long Kwok, Xin-Lei Ng, Zhen-Ann Loh, and Woon-Seng Gan </div> <div class="periodical"> <em>In Proceedings of the 13th Asia Pacific Signal and Information Processing Association Annual Summit and Conference </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ooi2021StronglyLabelledPolyphonicDataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		A {{Strongly-Labelled Polyphonic Dataset}} of {{Urban Sounds}} with
  		{{Spatiotemporal Context}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Kenneth and Watcharasupat, Karn N. and Peksi, Santi and Karnapi, Furi Andi and Ong, Zhen-Ting and Chua, Danny and Leow, Hui-Wen and Kwok, Li-Long and Ng, Xin-Lei and Loh, Zhen-Ann and Gan, Woon-Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 13th {{Asia Pacific Signal}} and {{Information Processing
  		Association Annual Summit}} and {{Conference}}
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Asia Pacific Signal and Information Processing Association}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Tokyo, Japan}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2021ControllableMusicSupervised" class="col-sm-10"> <div class="title"> Controllable Music: Supervised Learning of Disentangled Representations for Music Generation </div> <div class="author"> <em>Karn N. Watcharasupat</em> </div> <div class="periodical"> Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Controllability, despite being a much-desired property of a generative model, remains an ill-defined concept that is difficult to measure. In the context of neural music generation, a controllable system often implies an intuitive interaction between human agents and the neural model, allowing the relatively opaque neural model to be controlled by a human in a semantically understandable manner. In this work, we aim to tackle controllable music generation in the raw audio domain, which is significantly less attempted compared to the symbolic domain. Specifically, we focus on controlling multiple continuous, potentially interdependent timbral attributes of a musical note using a variational autoencoder (VAE) framework, and the necessary groundwork research needed to support the goal. Specifically, this work consists of three main parts. The first formulates the concept of }textit{controllability} and how to evaluate a latent manifold of deep generative models in the presence of multiple interdependent attributes. The second focuses on the development of a composite latent space architecture for VAE, in order to allow encoding of interdependent attributes which having an easily sampled disentangled prior. Proofs of concept work for the second part was performed on several standard vision disentanglement learning datasets. Finally, the last part applies the composite latent space model on music generation in the raw audio domain and discusses the evaluation of the model against the criteria defined in the first part of this project. All in all, given the relatively uncharted nature of the controllable generation in the raw audio domain, this project provides a foundational work for the evaluation of controllable generation as a whole, and a promising proof of concept for musical audio generation with timbral control using variational autoencoders. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">Watcharasupat2021ControllableMusicSupervised</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Controllable Music: Supervised Learning of Disentangled Representations for
  		Music Generation
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">type</span> <span class="p">=</span> <span class="s">{Final {{Year Project}} ({{FYP}})}</span><span class="p">,</span>
  <span class="na">lccn</span> <span class="p">=</span> <span class="s">{CY3001-211}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Nanyang Technological University}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2021EvaluationLatentSpace" class="col-sm-10"> <div class="title"> Evaluation of Latent Space Disentanglement in the Presence of Interdependent Attributes </div> <div class="author"> <em>Karn N. Watcharasupat</em>, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> <em>In Extended Abstracts of the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Watcharasupat2021EvaluationLatentSpace</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Evaluation of Latent Space Disentanglement in the Presence of Interdependent
  		Attributes
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Extended {{Abstracts}} of the {{Late-Breaking Demo Session}} of the 22nd
  		{{International Society}} for {{Music Information Retrieval Conference}}
  	}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Watcharasupat2021ImprovingPolyphonicSound" class="col-sm-10"> <div class="title"> Improving Polyphonic Sound Event Detection on Multichannel Recordings with the Sørensen-Dice Coefficient Loss and Transfer Learning </div> <div class="author"> <em>Karn N. Watcharasupat</em>, Thi Ngoc Tho Nguyen, Ngoc Khanh Nguyen, Zhen Jian Lee, Douglas L Jones, and Woon Seng Gan </div> <div class="periodical"> Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Watcharasupat2021ImprovingPolyphonicSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Improving {{Polyphonic Sound Event Detection}} on {{Multichannel Recordings}}
  		with the {{S{\o}rensen-Dice Coefficient Loss}} and {{Transfer Learning}}
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn N. and Nguyen, Thi Ngoc Tho and Nguyen, Ngoc Khanh and Lee, Zhen Jian and Jones, Douglas L and Gan, Woon Seng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="Wong2021AssessmentInterICSound" class="col-sm-10"> <div class="title"> Assessment of Inter-IC Sound Microelectromechanical Systems Microphones for Soundscape Reporting </div> <div class="author"> Trevor Wong, Bhan Lam, <em>Karn Watcharasupat</em>, Kenneth Ooi, Zhen-ting Ong, Furi Andi Karnapi, Woon-Seng Gan, Jooyoung Hong, Samuel Yeong, and Irene Lee </div> <div class="periodical"> <em>In Proceedings of the 50th International Congress and Expo on Noise Control Engineering </em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3397/in-2021-2086"></span> <span class="__dimensions_badge_embed__" data-doi="10.3397/in-2021-2086" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wong2021AssessmentInterICSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{
  		Assessment of Inter-{{IC}} Sound Microelectromechanical Systems Microphones
  		for Soundscape Reporting
  	}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wong, Trevor and Lam, Bhan and Watcharasupat, Karn and Ooi, Kenneth and Ong, Zhen-ting and Karnapi, Furi Andi and Gan, Woon-Seng and Hong, Jooyoung and Yeong, Samuel and Lee, Irene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{
  		Proceedings of the 50th {{International Congress}} and {{Expo}} on {{Noise
  		Control Engineering}}
  	}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{I-INCE}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Washington, D.C., USA}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3397/in-2021-2086}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div id="Watcharasupat2020VisualAttentionMusical" class="col-sm-10"> <div class="title">Visual Attention for Musical Instrument Recognition</div> <div class="author"> <em>Karn Watcharasupat</em>, Siddharth Gururani, and <a href="https://www.alexanderlerch.com/" rel="external nofollow noopener" target="_blank">Alexander Lerch</a> </div> <div class="periodical"> Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.09640" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-altmetric-id="true"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> In the field of music information retrieval, the task of simultaneously identifying the presence or absence of multiple musical instruments in a polyphonic recording remains a hard problem. Previous works have seen some success in improving instrument classification by applying temporal attention in a multi-instance multi-label setting, while another series of work has also suggested the role of pitch and timbre in improving instrument recognition performance. In this project, we further explore the use of attention mechanism in a timbral-temporal sense, la visual attention, to improve the performance of musical instrument recognition using weakly-labeled data. Two approaches to this task have been explored. The first approach applies attention mechanism to the sliding-window paradigm, where a prediction based on each timbral-temporal ‘instance’ is given an attention weight, before aggregation to produce the final prediction. The second approach is based on a recurrent model of visual attention where the network only attends to parts of the spectrogram and decide where to attend to next, given a limited number of ‘glimpses’. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Watcharasupat2020VisualAttentionMusical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Attention for Musical Instrument Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watcharasupat, Karn and Gururani, Siddharth and Lerch, Alexander}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Karn N. Watcharasupat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0CJ2WMKNQY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0CJ2WMKNQY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>